# ğŸ™ï¸ voice-age-predict (Common Voice)  
Fundamental ML project: **voice feature engineering + NN classifier** to predict **(Gender / Age / Gender+Age)** labels from speech.

- Dataset: Mozilla **Common Voice** (Kaggle mirror)  
- Core idea: **hand-crafted acoustic features** (MFCC / spectral / energy / F0 stats) â†’ **MLP classifier**  
- Outputs: trained model (`best_model.keras`) + top-k prediction on a single audio

---

## ğŸ“Œ What this project does
1) Build a feature table from raw audio files (Common Voice clips)  
2) Train a neural network classifier from the feature table  
3) Predict top-k labels for a given audio file (supports `.m4a` via ffmpeg convert)

> Codebase is organized as `preprocess.py` (feature extraction), `train.py` (training pipeline), `model.py` (MLP), `test.py` (inference).  
> (Reference: `preprocess.py`, `train.py`, `model.py`, `test.py`) 

---

## ğŸ§± Execution Pipeline (End-to-End)

```mermaid
flowchart TD
    A[Common Voice Audio Clips<br/>(.wav / .mp3 / .m4a ...)] --> B[Preprocess: Feature Extraction<br/>librosa + stats]
    B --> C[Feature Table<br/>CSV / XLSX<br/>[filename, gender, features..., label]]
    C --> D[Train: Stratified Split<br/>train / val / test]
    D --> E[Normalize Features<br/>StandardScaler<br/>(keep gender raw)]
    E --> F[MLP Classifier (Keras)<br/>Dense x N + BN + Dropout]
    F --> G[Best Model Checkpoint<br/>best_model.keras]
    G --> H[Test/Inference: Single Audio]
    H --> I[Optional: ffmpeg convert to WAV<br/>(for .m4a etc)]
    I --> J[Extract Features + gender_hint]
    J --> K[Scale with train-fitted scaler<br/>(keep gender raw)]
    K --> L[Predict Top-k Labels<br/>softmax probabilities]

    C -.-> M[Optional: Permutation Importance<br/>feature impact report]
'''

âœ… Pipeline Notes (ê·¸ë¦¼ ì„¤ëª…)

Preprocess ë‹¨ê³„: librosaë¡œ ìŒì„±ì„ ë¡œë“œí•œ ë’¤, ìŠ¤í™íŠ¸ëŸ¼/ì—ë„ˆì§€/F0/MFCC ê¸°ë°˜ í†µê³„ íŠ¹ì§•ì„ ë½‘ì•„ í•œ ì¤„(feature vector) ë¡œ ë§Œë“  ë‹¤ìŒ CSV/XLSX í…Œì´ë¸”ë¡œ ì €ì¥í•œë‹¤. 


Train ë‹¨ê³„: feature tableì„ ë¡œë“œâ†’ì»¬ëŸ¼ ì •ê·œí™”â†’ë¼ë²¨ ê¸°ì¤€ stratified splitâ†’StandardScalerë¡œ ìŠ¤ì¼€ì¼ë§í•˜ë˜ gender(ì²« ì—´)ëŠ” ì›ê°’ ìœ ì§€â†’MLP í•™ìŠµâ†’ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ best_model.kerasë¡œ ì €ì¥í•œë‹¤.

Test ë‹¨ê³„: ë‹¨ì¼ ì˜¤ë””ì˜¤ ì…ë ¥ì„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œí•˜ê³ , í•™ìŠµì—ì„œ ë§Œë“  ìŠ¤ì¼€ì¼ ê·œì¹™ì„ ì ìš©í•œ ë’¤ softmax í™•ë¥  top-kë¥¼ ì¶œë ¥í•œë‹¤. .m4a ë“±ì€ ffmpegë¡œ ì„ì‹œ WAV ë³€í™˜ì„ ì§€ì›í•œë‹¤. 


Feature Importance(ì„ íƒ): ê²€ì¦ì…‹ì—ì„œ ì»¬ëŸ¼ì„ í•˜ë‚˜ì”© ì…”í”Œí•´ ì •í™•ë„ í•˜ë½(Î”acc)ì„ ì¸¡ì •í•˜ëŠ” Permutation Importanceë¡œ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ë½‘ì„ ìˆ˜ ìˆë‹¤.

ğŸ§  Model Architecture (MLP Classifier)

Input is a 111-D vector = [gender_code(1)] + [acoustic_features(110)]

acoustic_features = 3(spectral) + 25Ã—2(MFCC mean/std) + 25Ã—2(Î”MFCC mean/std) + 4(RMS stats) + 3(F0 stats) = 110

```mermaid
flowchart LR
    X[Input Vector<br/>111 dims<br/>(gender + features)] --> BN0[BatchNorm]

    BN0 --> D1[Dense 1024 + ReLU] --> BN1[BatchNorm] --> DP1[Dropout 0.2]
    DP1 --> D2[Dense 1024 + ReLU] --> BN2[BatchNorm] --> DP2[Dropout 0.1]
    DP2 --> D3[Dense 1024 + ReLU] --> BN3[BatchNorm] --> DP3[Dropout 0.2]
    DP3 --> D4[Dense 1024 + ReLU] --> BN4[BatchNorm] --> DP4[Dropout 0.1]
    DP4 --> D5[Dense 1024 + ReLU] --> BN5[BatchNorm] --> DP5[Dropout 0.2]
    DP5 --> D6[Dense 1024 + ReLU] --> BN6[BatchNorm]

    BN6 --> OUT[Dense = num_classes<br/>Softmax]
'''

âœ… Model Notes (ê·¸ë¦¼ ì„¤ëª…)

ì´ ëª¨ë¸ì€ CNN/RNN ì—†ì´ â€œíŠ¹ì§•ê³µí•™ + MLPâ€ë¡œ ëë‚´ëŠ” êµ¬ì¡°ë‹¤.

ì…ë ¥ì€ [gender_code] + [ìŒí–¥ í†µê³„ íŠ¹ì§•]ì´ê³ , ì—¬ëŸ¬ ì¸µì˜ Dense(1024) + BN + Dropoutì„ ë°˜ë³µí•´ ë¹„ì„ í˜• ê²°í•©ì„ í•™ìŠµí•œë‹¤.

ì¶œë ¥ì€ softmax(num_classes)ì´ë©°, í´ë˜ìŠ¤ ìˆ˜ëŠ” í•™ìŠµ ë¼ë²¨(ì˜ˆ: Female_twentieth, Male_thirties ë“±)ì˜ ìœ ë‹ˆí¬ ê°œìˆ˜ë¡œ ìë™ ê²°ì •ëœë‹¤.
